# -*- coding: utf-8 -*-
"""SpotCheck_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17rN6Q1DQfdk7OkyEt9Ahzks7neL33nIX
"""

!apt-get install poppler-utils
!pip install pdf2image pytesseract pillow opencv-python

import os
import cv2
import numpy as np
from PIL import Image
import pytesseract
from collections import defaultdict
from pdf2image import convert_from_path

import os
from pdf2image import convert_from_path

def convert_pdf_to_images(pdf_path, output_image_folder):
    os.makedirs(output_image_folder, exist_ok=True)

    # Get filename without extension for naming
    base_name = os.path.splitext(os.path.basename(pdf_path))[0]

    # Convert all pages to images
    images = convert_from_path(pdf_path, dpi=300)

    for idx, image in enumerate(images):
        image_filename = f"{base_name}_page_{idx+1}.png"
        output_path = os.path.join(output_image_folder, image_filename)
        image.save(output_path, "PNG")
        print(f"Saved image: {output_path}")

convert_pdf_to_images("/content/pdfs/1900_spotCheck.pdf", "/content/pdf_images")

def preprocess_image(image_path):
    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if image is None:
        print(f"Failed to load: {image_path}")
        return None

    # Resize
    image = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)

    # Contrast
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    enhanced = clahe.apply(image)

    # Blur and threshold
    blurred = cv2.GaussianBlur(enhanced, (3, 3), 0)
    _, otsu = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

    # Morphological clean-up
    kernel_open = np.ones((2, 2), np.uint8)
    opened = cv2.morphologyEx(otsu, cv2.MORPH_OPEN, kernel_open)
    kernel_dilate = np.ones((2, 1), np.uint8)
    dilated = cv2.dilate(opened, kernel_dilate, iterations=1)

    return dilated

def detect_and_crop_columns(image_path, output_dir, base_filename, min_height_ratio=0.2):
    os.makedirs(output_dir, exist_ok=True)

    # Load image in grayscale
    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if image is None:
        print(f"‚ùå Cannot load: {image_path}")
        return []

    height, width = image.shape
    min_height = int(min_height_ratio * height)

    # Binarize
    _, binary = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    # Vertical line detection
    vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, height // 20))
    vertical_lines = cv2.morphologyEx(binary, cv2.MORPH_OPEN, vertical_kernel, iterations=1)

    # Find contours and filter by height
    contours, _ = cv2.findContours(vertical_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    column_boxes = [cv2.boundingRect(c) for c in contours]
    tall_boxes = [box for box in column_boxes if box[3] >= min_height]
    tall_boxes.sort(key=lambda b: b[0])

    x_coords = [x for (x, y, w, h) in tall_boxes]
    print(f"Tall vertical line x-positions: {x_coords}")

    if not x_coords:
        print(" No tall vertical lines found.")
        return []

    # Find the line closest to center
    center_x = width // 2
    x_cut = min(x_coords, key=lambda x: abs(x - center_x))
    print(f"‚úÖ Splitting at x={x_cut}, nearest to center x={center_x}")

    cropped_paths = []

    # Left column
    left_crop = image[:, 0:x_cut]
    left_path = os.path.join(output_dir, f"{base_filename}_left.png")
    cv2.imwrite(left_path, left_crop)
    cropped_paths.append(left_path)

    # Right column
    right_crop = image[:, x_cut:width]
    right_path = os.path.join(output_dir, f"{base_filename}_right.png")
    cv2.imwrite(right_path, right_crop)
    cropped_paths.append(right_path)

    return cropped_paths

def ocr_columns(cropped_paths):
    full_text = ""
    for idx, img_path in enumerate(sorted(cropped_paths)):
        img = cv2.imread(img_path)
        if img is None:
            continue
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        column_text = pytesseract.image_to_string(binary, config="--psm 4")
        full_text += f"\n### Column {idx + 1} ({os.path.basename(img_path)}) ###\n{column_text.strip()}\n"
    return full_text

import os
from collections import defaultdict

def process_all_years(input_folder="/content/pdf_images",
                      output_text_folder="/content/ocr_texts",
                      column_crop_root="/content/column_crops"):

    os.makedirs(output_text_folder, exist_ok=True)
    os.makedirs(column_crop_root, exist_ok=True)

    year_to_images = defaultdict(list)

    # Step 1: Group images by year
    for image_file in sorted(os.listdir(input_folder)):
        if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):
            year = image_file.split("_")[0]
            year_to_images[year].append(os.path.join(input_folder, image_file))
            print(f"Found image: {image_file} (Year: {year})")

    print(f"\nDetected years: {list(year_to_images.keys())}\n")

    # Step 2: Process each year
    for year, image_paths in year_to_images.items():
        print(f"Processing year: {year} ({len(image_paths)} pages)")

        for img_path in sorted(image_paths):
            img_name = os.path.basename(img_path)
            parts = img_name.split("_")
            if len(parts) >= 3:
                page_id = parts[3].split(".")[0]  # e.g., 1 from 1900_batch_1.png
            else:
                page_id = "unknown"

            print(f"  Processing page: {img_name}")

            # Step 2a: Preprocess image
            preprocessed = preprocess_image(img_path)
            if preprocessed is None:
                print(f"  Skipping {img_name} (preprocessing failed)")
                continue

            # Step 2b: Detect and crop columns, save to page-specific folder
            column_output_dir = os.path.join(column_crop_root, f"{year}_page_{page_id}")
            os.makedirs(column_output_dir, exist_ok=True)

            cropped_columns = detect_and_crop_columns(
                img_path,
                output_dir=column_output_dir,
                base_filename=f"{year}_page_{page_id}"
            )

            if not cropped_columns:
                print(f"  No columns detected on {img_name}. Skipping OCR.")
                continue

            # Step 2c: OCR on cropped columns
            page_text = ocr_columns(cropped_columns)

            # Step 2d: Save OCR text for this page
            output_text_path = os.path.join(output_text_folder, f"{year}_page_{page_id}.txt")
            with open(output_text_path, "w", encoding="utf-8") as f:
                f.write(page_text)

            print(f"  Saved OCR text to {output_text_path}")

        print(f"Completed processing for year: {year}\n")

process_all_years()

!pip uninstall google-generativeai -y
!pip install --no-cache-dir google-generativeai

import google.generativeai as genai

genai.configure(api_key="")

model = genai.GenerativeModel('gemini-1.5-flash')

import google.generativeai as genai
import json
import re

# ===  Read OCR Text and Chunk ===
def read_and_chunk_file(file_path, chunk_size=1500):
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()

    text = re.sub(r'\n+', '\n', text.strip())
    lines = text.split('\n')

    chunks = []
    current = []
    length = 0

    for line in lines:
        current.append(line)
        length += len(line)
        if length >= chunk_size:
            chunks.append('\n'.join(current))
            current = []
            length = 0

    if current:
        chunks.append('\n'.join(current))

    return chunks

# === Gemini Prompt: Extract ALL Persons from Chunk ===
def extract_all_people_from_chunk(chunk, directory_name="Minneapolis 1900", page_number=104):
    prompt = f"""
From the following historical directory OCR text, extract ALL person records and return them as a JSON array.
Each person object should follow this format:

{{
  "FirstName": "...",
  "LastName": "...",
  "Spouse": "...",
  "Occupation": "...",
  "CompanyName": "...",
  "HomeAddress": {{
    "StreetNumber": "...",
    "StreetName": "...",
    "ApartmentOrUnit": "...",
    "ResidenceIndicator": "h" | "w" | null
  }},
  "WorkAddress": null,
  "Telephone": null,
  "DirectoryName": "{directory_name}",
  "PageNumber": {page_number}
}}

If no people are found in the chunk, return an empty list: `[]`.

OCR TEXT:
\"\"\"
{chunk}
\"\"\"
"""

    try:
        response = model.generate_content(prompt)
        content = response.text.strip()

        if content.startswith("```json"):
            content = content.split("```json")[1].split("```")[0].strip()

        data = json.loads(content)
        if isinstance(data, list):
            return data
        return []
    except Exception as e:
        print(" Could not parse Gemini output:", e)
        print("Response was:\n", response.text)
        return []

# ===  Loop Over Chunks and Collect ALL People ===
def extract_all_people(file_path):
    chunks = read_and_chunk_file(file_path)
    all_people = []

    for i, chunk in enumerate(chunks):
        print(f"üîç Processing chunk {i+1}/{len(chunks)}...")
        people = extract_all_people_from_chunk(chunk)
        all_people.extend(people)

    return all_people

# ===  Save Results ===
def save_to_json(data, output_file='extracted_people.json'):
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"\n Extracted {len(data)} people. Saved to '{output_file}'.")

# === MAIN ===
import os

if __name__ == "__main__":
    all_people = []

    ocr_folder = "/content/ocr_texts"
    for txt_file in sorted(os.listdir(ocr_folder)):
        if txt_file.endswith(".txt"):
            txt_path = os.path.join(ocr_folder, txt_file)
            print(f"Processing OCR file: {txt_file}")
            people = extract_all_people(txt_path)
            all_people.extend(people)

    save_to_json(all_people)

